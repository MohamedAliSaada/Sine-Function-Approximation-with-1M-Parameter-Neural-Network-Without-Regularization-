# -*- coding: utf-8 -*-
"""Regularization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pDOt_j9kVyYfCAe2ZtT6PDgBJqPcRkkH
"""

#import lib we need .

import tensorflow as tf
from tensorflow import keras
from keras.layers import Dense
from keras.models import Sequential

from sklearn.preprocessing import StandardScaler


import numpy as np
import matplotlib.pyplot as plt

import random

#fix all possible random
seed=0
np.random.seed(seed)
random.seed(seed)
tf.random.set_seed(seed)

# Generate the data
x_train = np.linspace(-100, 100, 5000).reshape(-1, 1)



# Define the function for y_train
def my_fun(x):
    return np.sin(x)
# Compute y_train using the unnormalized x_train_
y_train= my_fun(x_train_)


# Check for NaN and Inf in y_train
print(np.isnan(y_train).sum())  # Should be 0 if no NaNs
print(np.isinf(y_train).sum())  # Should be 0 if no Infs

# Build the model
Module = Sequential([
    Dense(1024, activation='relu', input_shape=(x_train.shape[1],)),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(1, activation='linear')
])

# Compile the model
Module.compile(
    optimizer='adam',
    loss='mean_squared_error',  # Appropriate for regression
    metrics=['mean_absolute_error']  # Suitable metric for regression
)

# Fit the model
history = Module.fit(
    x_train, y_train,
    epochs=30,
    #batch_size=256,
    validation_split=0.1  # 10% of the data is used for validation
)

# Plot the results
plt.figure(figsize=(12, 5))

# MAE plot
plt.subplot(2, 2, 1)
plt.plot(history.history['mean_absolute_error'], label='Training MAE', color='r')
#plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE', color='b')
plt.legend()
plt.grid(True)
plt.xlabel("Epochs")
plt.ylabel("Mean Absolute Error")

plt.subplot(2, 2, 2)
plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE', color='b')
plt.legend()
plt.grid(True)
plt.xlabel("Epochs")
plt.ylabel("Mean Absolute Error")


# Loss plot
plt.subplot(2, 2, 3)
plt.plot(history.history['loss'], label='Training Loss', color='r')
#plt.plot(history.history['val_loss'], label='Validation Loss', color='b')
plt.legend()
plt.grid(True)
plt.xlabel("Epochs")
plt.ylabel("Mean Squared Error Loss")

plt.subplot(2, 2, 4)
plt.plot(history.history['val_loss'], label='Validation Loss', color='b')
plt.legend()
plt.grid(True)
plt.xlabel("Epochs")
plt.ylabel("Mean Squared Error Loss")


plt.tight_layout()
plt.show()

# Scale x_test using the same scaler that was used for x_train (scaler_x)
x_test = np.linspace(-200, 200, 5000).reshape(-1, 1)


# Predict using the trained model
y_predict = Module.predict(x_test)


# Plotting the results
plt.figure(figsize=(12, 6))
plt.plot(x_train, y_train, label='Training Data (True Values)', color='r')
plt.plot(x_test, y_predict, label='Predicted Values', color='b')

# Adding labels and grid
plt.legend()
plt.grid(True)
plt.xlabel("x")
plt.ylabel("y")

plt.tight_layout()
plt.show()

Module.summary()

